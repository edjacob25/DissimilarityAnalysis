{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import scipy as sp\n",
    "from jmetal.lab.statistical_test.critical_distance import CDplot\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import git\n",
    "from scipy.stats import wilcoxon\n",
    "from sqlalchemy import create_engine, or_, Column, Integer, String, Float, DateTime, ForeignKey, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from data_types import Experiment, ExperimentSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_db_to_pandas(query, eliminate_classes_different=False):\n",
    "    last = \"\"\n",
    "    headers = []\n",
    "    datasets = []\n",
    "    series = []\n",
    "    serie = None\n",
    "    code_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "    repo = git.Repo(code_directory)\n",
    "\n",
    "    for experiment in query:\n",
    "        filename = experiment.file_name.rsplit('/')[-1].split(\".\")[0]\n",
    "        if last != filename:\n",
    "            last = filename\n",
    "            dataset_name = filename\n",
    "            datasets.append(dataset_name)\n",
    "            if serie is not None:\n",
    "                series.append(serie)\n",
    "            serie = []\n",
    "        message = experiment.set.description\n",
    "        measure =  experiment.method.split('.')[-1].replace(\"None\", \"\").replace(\"-R first-last \", \"\").replace(\" -S\",  \"\").replace(\" -W\",  \"\").replace(\"_\", \" \").strip()\n",
    "        header = measure\n",
    "            \n",
    "        if header not in headers:\n",
    "            headers.append(header)\n",
    "        if eliminate_classes_different and (experiment.number_of_classes is None or experiment.number_of_clusters != experiment.number_of_classes):\n",
    "            serie.append(0.0)\n",
    "        else:\n",
    "            serie.append(experiment.f_score)\n",
    "    \n",
    "    series.append(serie)\n",
    "    # print(headers)\n",
    "    # print(len(headers))\n",
    "    \n",
    "    return pd.DataFrame(series, index = datasets, columns=headers)\n",
    "\n",
    "def order_dataset(df):\n",
    "    return df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "def get_dataset(database):\n",
    "    engine = create_engine(database, echo=False)\n",
    "    session_class = sessionmaker(bind=engine)\n",
    "    session = session_class()\n",
    "    query = session.query(Experiment).order_by(Experiment.file_name, Experiment.id)\n",
    "    #query = for experiment in session.query(Experiment).filter(or_(Experiment.set_id==i for i in [17])).order_by(Experiment.file_name):\n",
    "    #query = for experiment in session.query(Experiment).filter(Experiment.number_of_clusters == Experiment.number_of_classes).order_by(Experiment.file_name):\n",
    "    return order_dataset(from_db_to_pandas(query))\n",
    "\n",
    "def get_averages(df):\n",
    "    dft = df.apply(lambda x: x.rank(ascending=False), axis = 1)\n",
    "    averages = dft.mean() \n",
    "    #averages = averages.sort_values()\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset('sqlite:///results_testingDatasets.db')\n",
    "df2 = get_dataset('sqlite:///results_extensive.db')\n",
    "rand = get_dataset('sqlite:///results_rand.db')\n",
    "adjusted_rand = get_dataset('sqlite:///results_adjusted_rand.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = [x for x in df2.columns if x in df.columns and x in rand.columns]\n",
    "common_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = df.loc[:, common_cols].append(df2.loc[:, common_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_avg = get_averages(auc)\n",
    "rand_avg = get_averages(rand)\n",
    "ad_rand_avg = get_averages(adjusted_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "CDplot(auc.transpose(), higher_is_better=True, alpha=0.1, output_filename='auc.png')\n",
    "CDplot(rand.transpose(), higher_is_better=True, alpha=0.1, output_filename='rand.png')\n",
    "CDplot(adjusted_rand.transpose(), higher_is_better=True, alpha=0.1, output_filename='adjusted_rand.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(auc_avg.index, auc_avg, label=\"auc\", s=20*4*2, marker=\"*\")\n",
    "ax.scatter(auc_avg.index, rand_avg, label=\"rand\", s=20*4*2, marker=\"+\")\n",
    "ax.scatter(auc_avg.index, ad_rand_avg, label=\"adjusted_rand\", s=20*4*2, marker=\".\")\n",
    "ax.set_ylabel('Average position')\n",
    "ax.set_xlabel('Measures')\n",
    "ax.legend()\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_individually(header, df):\n",
    "    pairs = [x for x in df.columns if x != header ]\n",
    "    for y in pairs:\n",
    "        c, p = wilcoxon(df[header], df[y], alternative=\"greater\")\n",
    "        if p < 0.1:    \n",
    "            print(f\"{header} vs {y} -> {p}\")\n",
    "        else:\n",
    "            print(f\"{header} and {y} are not different, p -> {p}\")\n",
    "\n",
    "compare_individually(\"LinModified KappaMax\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
