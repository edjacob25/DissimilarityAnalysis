{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import scipy as sp\n",
    "from jmetal.lab.statistical_test.critical_distance import CDplot\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import git\n",
    "from scipy.stats import wilcoxon\n",
    "from sqlalchemy import create_engine, or_, Column, Integer, String, Float, DateTime, ForeignKey, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from do_analysis import Experiment, ExperimentSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_db_to_pandas(query):\n",
    "    last = \"\"\n",
    "    headers = []\n",
    "    datasets = []\n",
    "    series = []\n",
    "    serie = None\n",
    "    code_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "    repo = git.Repo(code_directory)\n",
    "\n",
    "    for experiment in query:\n",
    "        if last != experiment.file_name:\n",
    "            last = experiment.file_name\n",
    "            dataset_name = experiment.file_name.rsplit('/')[-1].split(\".\")[0]\n",
    "            datasets.append(dataset_name)\n",
    "            if serie is not None:\n",
    "                series.append(serie)\n",
    "            serie = []\n",
    "        message = experiment.set.description\n",
    "        measure =  experiment.method.split('.')[-1]\n",
    "       \n",
    "        if \"Initial\" not in experiment.set.description:\n",
    "            header = measure.replace('Modified','')\n",
    "        else:\n",
    "            header = measure\n",
    "        \n",
    "        if experiment.method == \"weka.core.LearningBasedDissimilarity -R first-last -S E -W K\":\n",
    "            continue\n",
    "            \n",
    "        if header not in headers:\n",
    "            headers.append(header)\n",
    "        if experiment.number_of_classes is None or experiment.number_of_clusters != experiment.number_of_classes:\n",
    "            serie.append(0.0)\n",
    "        else:\n",
    "            serie.append(experiment.f_score)\n",
    "    \n",
    "    series.append(serie)\n",
    "    print(len(headers))\n",
    "    \n",
    "    return pd.DataFrame(series, index = datasets, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///../results_testing.db', echo=False)\n",
    "session_class = sessionmaker(bind=engine)\n",
    "session = session_class()\n",
    "query = session.query(Experiment).order_by(Experiment.file_name, Experiment.id)\n",
    "#query = for experiment in session.query(Experiment).filter(or_(Experiment.set_id==i for i in [17])).order_by(Experiment.file_name):\n",
    "#query = for experiment in session.query(Experiment).filter(Experiment.number_of_clusters == Experiment.number_of_classes).order_by(Experiment.file_name):\n",
    "df = from_db_to_pandas(query)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.apply(lambda x: x.rank(ascending=False), axis = 1)\n",
    "dft\n",
    "averages = dft.mean() \n",
    "averages = averages.sort_values()\n",
    "averages.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDplot(df.transpose(), higher_is_better=True, alpha=0.1, output_filename='cdplotss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = \"Lin_KappaMax\"\n",
    "pairs = [x for x in df.columns if x != lb ]\n",
    "for y in pairs:\n",
    "    c, p = wilcoxon( df[lb], df[y], alternative=\"greater\")\n",
    "    if p < 0.1:    \n",
    "        print(f\"{lb} vs {y} -> {p}\")\n",
    "    else:\n",
    "        print(f\"{lb} and {y} are not different, p -> {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finner_test_mine(ranks, control=None):\n",
    "    k = len(ranks)\n",
    "    values = list(ranks.values())\n",
    "    keys = list(ranks.keys())\n",
    "    if not control :\n",
    "        control_i = values.index(min(values))\n",
    "    else:\n",
    "        control_i = keys.index(control)\n",
    "\n",
    "    comparisons = [keys[control_i] + \" vs \" + keys[i] for i in range(k) if i != control_i]\n",
    "    z_values = [abs(values[control_i] - values[i]) for i in range(k) if i != control_i]\n",
    "    p_values = [2*(1-st.norm.cdf(abs(z))) for z in z_values]\n",
    "    # Sort values by p_value so that p_0 < p_1\n",
    "    p_values, z_values, comparisons = map(list, zip(*sorted(zip(p_values, z_values, comparisons), key=lambda t: t[0])))\n",
    "    adj_p_values = [min(max(1-(1-p_values[j])**((k-1)/float(j+1)) for j in range(i+1)), 1) for i in range(k-1)]\n",
    "    \n",
    "    return comparisons, z_values, p_values, adj_p_values\n",
    "\n",
    "\n",
    "def friedman_test_mine(rankings):\n",
    "    n = rankings.shape[0]\n",
    "    k = rankings.shape[1]\n",
    "    rankings_avg = rankings.mean()\n",
    "    rankings_cmp = [r/sp.sqrt(k*(k+1)/(6.*n)) for r in rankings_avg]\n",
    "\n",
    "    chi2 = ((12*n)/float((k*(k+1))))*((sp.sum(r**2 for r in rankings_avg))-((k*(k+1)**2)/float(4)))\n",
    "    iman_davenport = ((n-1)*chi2)/float((n*(k-1)-chi2))\n",
    "\n",
    "    p_value = 1 - st.f.cdf(iman_davenport, k-1, (k-1)*(n-1))\n",
    "\n",
    "    return iman_davenport, p_value, rankings_avg, rankings_cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friedman_res = friedman_test_mine(dft)\n",
    "f_value, p_value, rankings, pivots = friedman_res\n",
    "names = df.columns\n",
    "avg_dic = {name: pivot for name, pivot in zip(names, pivots)}\n",
    "t = finner_test_mine(avg_dic)\n",
    "comparisons, z_values, p_values, adjusted_p_values = t\n",
    "#t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_dic = {}\n",
    "for i in dft.columns:\n",
    "    values = [x for x in dft[i]]\n",
    "    dft_dic[i] = values\n",
    "\n",
    "avg_dic = averages.to_dict()\n",
    "avg_dic\n",
    "#dft_dic.keys()\n",
    "t = finner_test_mine(avg_dic)\n",
    "comparisons, z_values, p_values, adjusted_p_values = t\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
