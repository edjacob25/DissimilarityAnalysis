{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import git\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from scipy.stats import wilcoxon\n",
    "from sqlalchemy import create_engine, or_, Column, Integer, String, Float, DateTime, ForeignKey, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import relationship, sessionmaker\n",
    "from do_analysis import Experiment, ExperimentSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///results.db')\n",
    "session_class = sessionmaker(bind=engine)\n",
    "session = session_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = \"\"\n",
    "headers = []\n",
    "datasets = []\n",
    "series = []\n",
    "serie = None\n",
    "code_directory = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "repo = git.Repo(code_directory)\n",
    "\n",
    "for experiment in session.query(Experiment).order_by(Experiment.file_name, Experiment.id):\n",
    "#for experiment in session.query(Experiment).filter(or_(Experiment.set_id==i for i in [1])).order_by(Experiment.file_name):\n",
    "#for experiment in session.query(Experiment).filter(Experiment.number_of_clusters == Experiment.number_of_classes).order_by(Experiment.file_name):\n",
    "    if last != experiment.file_name:\n",
    "        last = experiment.file_name\n",
    "        dataset_name = experiment.file_name.rsplit('/')[-1].split(\".\")[0]\n",
    "        datasets.append(dataset_name)\n",
    "        if serie is not None:\n",
    "            series.append(serie)\n",
    "        serie = []\n",
    "    message = experiment.set.description\n",
    "    measure =  experiment.method.split('.')[-1]\n",
    "    if \"LearningBased\" in measure:\n",
    "        measure = measure.replace(\"LearningBasedDissimilarity\", \"LearningBased\")\n",
    "        measure = measure.replace(\"-R first-last \", \"\")\n",
    "        measure = measure.replace(\"-S\", \"strategy\")\n",
    "        measure = measure.replace(\" -W\", \", multiplier weight\")\n",
    "        measure = measure.replace(\" -n\", \", normalized\")\n",
    "        measure = measure.replace(\" -w\", \", decision weight\")\n",
    "        measure = measure.replace(\" -o\", \", multiplier strategy\")\n",
    "        measure = measure.replace(\" -t\", \", multiplier\")\n",
    "\n",
    "    header = f\"{measure}\"\n",
    "    if header not in headers:\n",
    "        headers.append(header)\n",
    "    if experiment.number_of_classes is None or experiment.number_of_clusters != experiment.number_of_classes:\n",
    "        serie.append(0.0)\n",
    "        #print(measure)\n",
    "    else:\n",
    "        serie.append(experiment.f_score)\n",
    "\n",
    "\n",
    "series.append(serie)\n",
    "\n",
    "df = pd.DataFrame(series, index = datasets, columns=headers)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank each value against the other options, per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = df.apply(lambda x: x.rank(ascending=False), axis = 1)\n",
    "dft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the mean rank per measure and order them from best to worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = dft.mean()\n",
    "averages = averages.sort_values()\n",
    "averages.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looking_for = [\"D, weight A - Learning Based using A as\", \"D, weight K - Learning Based using K as\", \n",
    "               \"E, weight N\", \"D, weight A - Learning Based using K as\", \n",
    "               \"D, weight K - Learning Based using A as\", \"D, weight N\"]\n",
    "\n",
    "resultant_headers = []\n",
    "for config in looking_for:\n",
    "    for average in averages.index:\n",
    "        if config in average:\n",
    "            resultant_headers.append(average)\n",
    "\n",
    "#resultant_headers\n",
    "df2 = df[resultant_headers]\n",
    "#df2.mean().sort_values()\n",
    "df2.apply(lambda x: x.rank(ascending=False), axis = 1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the best and second best using the Wilcoxon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = averages[resultant_headers].sort_values().index[1]\n",
    "#second_best = averages.sort_values().index[1]\n",
    "second_best = averages[resultant_headers].sort_values().index[7]\n",
    "print(best)\n",
    "print(second_best)\n",
    "\n",
    "c, p = wilcoxon( df[best], df[second_best], alternative=\"greater\")\n",
    "print(f\"{c} --- {p}\")\n",
    "c, p = wilcoxon( df[second_best], df[best], alternative=\"less\")\n",
    "print(f\"{c} --- {p}\")\n",
    "\n",
    "diff = [x - y for x, y in zip(df[best], df[second_best])]\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the differences between the best and second best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{best} | {second_best}\")\n",
    "for i, (x, y) in enumerate(zip(df[best], df[second_best])):\n",
    "    if x != y: \n",
    "        print(f\"{df.index[i]}: {x} - {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_upper(s):\n",
    "    return \"\".join(c for c in s if c.isupper())\n",
    "\n",
    "\n",
    "originals = []\n",
    "for av in averages.index:\n",
    "    #print(av)\n",
    "    if \"Original measures\" == av.split(\" - \")[1]:\n",
    "        name = av.split(\" \")[0]\n",
    "        if \"OccurenceFrequency\" in av:\n",
    "            name = only_upper(name)\n",
    "        originals.append((name, averages[av]))\n",
    "originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_than = {}\n",
    "for av in averages.index:\n",
    "    measure = f\"{av.split(' ')[0]}\"\n",
    "    for o_name, o_value in originals:\n",
    "        if f\"{o_name}Modified\" == measure:\n",
    "            if o_value > averages[av]:\n",
    "                if o_name not in better_than:\n",
    "                    better_than[o_name] = []\n",
    "                better_than[o_name].append((av, averages[av]))\n",
    "better_than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_than = {}\n",
    "to_compare = \"Original measures times kappa\"\n",
    "for i, average in enumerate(averages):\n",
    "    if averages.index[i].split(\" - \")[1] == to_compare:\n",
    "        measure = f\"{avre.split(' ')[0]}\"\n",
    "        print(measure)\n",
    "        for o_name, o_value in originals:\n",
    "            if f\"{o_name}Modified\" == measure:\n",
    "                if o_value > averages[av]:\n",
    "                    if o_name not in better_than:\n",
    "                        better_than[o_name] = []\n",
    "                    better_than[o_name].append((av, averages[av]))\n",
    "better_than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_colors = list(plt.cm.colors.cnames.keys())\n",
    "random.seed(1000)\n",
    "c = random.choices(all_colors, k=125)\n",
    "fig = plt.figure(figsize=(16,10), dpi= 80)\n",
    "ax = fig.add_subplot(111)\n",
    "#fig, ax = plt.subplots()\n",
    "#box = df.boxplot(ax=ax)\n",
    "\n",
    "bp = ax.boxplot(df.transpose(), autorange=True, widths=0.65, patch_artist=True)\n",
    "ax.margins(y=0.05)\n",
    "\n",
    "for label in (plt.gca().get_yticklabels()):\n",
    "    label.set_fontsize(18) # Size here overrides font_prop\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    # change outline color\n",
    "    #box.set( color='#7570b3', linewidth=2)\n",
    "    # change fill color\n",
    "    box.set_facecolor(c[i])\n",
    "    pass\n",
    "for median in bp['medians']:\n",
    "    median.set(color='black')\n",
    "    \n",
    "for i, val in enumerate(averages.values):\n",
    "    plt.text(i+1, val, \"{0:.4f}\".format(val),horizontalalignment='center', verticalalignment='bottom', fontdict={'fontweight':500, 'size':18})\n",
    "    \n",
    "plt.ylabel('F1-Score', fontsize=20)\n",
    "plt.gca().set_xticklabels(df.index, rotation=60, horizontalalignment= 'right', fontdict={'fontweight':500, 'size':18})\n",
    "plt.savefig(\"box.png\", transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reindex(dft.mean().sort_values().index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ours = \"LearningBased strategy D, weight A - Learning Based using A\"\n",
    "measures = [\"Eskin\", \"Gambaryan\", \"Goodall\", \"OccurenceFrequency\", \"InverseOccurenceFrequency\", \"Lin\"]\n",
    "for measure in measures:\n",
    "    t, p = wilcoxon(df[ours], df[measure], alternative=\"greater\")\n",
    "    print(f\"Ours vs {measure}, pvalue {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[ours] - df[measures[1]])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
